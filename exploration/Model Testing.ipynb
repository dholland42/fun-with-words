{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from fun_with_words import data as fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make basic layers needed for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def call(self, inputs):\n",
    "        return tf.strings.split(inputs)\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (*input_shapes, None)\n",
    "\n",
    "class CharacterSplitter(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def call(self, inputs):\n",
    "        return tf.strings.bytes_split(inputs).to_tensor()\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (*input_shapes, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = string.ascii_letters + string.digits + string.punctuation\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocabulary=VOCAB, OOV=\"OOV\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocabulary = vocabulary\n",
    "        self.lookup = tf.lookup.StaticHashTable(\n",
    "            initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=[\"OOV\"] + [v for v in vocabulary],\n",
    "                values=list(range(1, len(vocabulary) + 2)),\n",
    "            ),\n",
    "            default_value=0\n",
    "        )\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.lookup.lookup(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update(dict(vocabulary=self.vocabulary))\n",
    "        return cfg\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_level_word_embedding(filters=(3, 5, 7), vocabulary=VOCAB, OOV=\"OOV\", name=\"char_level_word_embedding\", **kwargs):\n",
    "    inp = tf.keras.Input(shape=(), dtype=tf.string)\n",
    "    t = Tokenizer(name=\"tokenizer\")\n",
    "    c = CharacterSplitter(name=\"character_splitter\")\n",
    "    e = Encoder(name=\"encoder\")\n",
    "    emb = tf.keras.layers.Embedding(input_dim=len(e.lookup.export()[0]) + 1, output_dim=100)\n",
    "    flayers = [tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(filters=100, kernel_size=f, padding=\"same\")) for f in filters]\n",
    "    \n",
    "    # push input through the layers\n",
    "    embedded = emb(e(c(t(inp))))\n",
    "    filtered = [f(embedded) for f in flayers]\n",
    "    pooled = [tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalMaxPooling1D())(f) for f in filtered]\n",
    "    out = (tf.keras.layers.Concatenate()(pooled))\n",
    "    model = tf.keras.Model(inp, out, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_layer = tf.keras.Input(shape=(), dtype=tf.string)\n",
    "        self.embedding = char_level_word_embedding(name=\"word_embedding\", input_shape=(None,))\n",
    "        self.lm = lm = tf.keras.layers.LSTM(1024, name=\"language_model\")\n",
    "        self.d = tf.keras.layers.Dense(300, name=\"dense_out\")\n",
    "        self.out = self.call(self.input_layer)\n",
    "        super().__init__(\n",
    "            inputs=self.input_layer,\n",
    "            outputs=self.out,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        emb = self.embedding(inputs)\n",
    "        lm = self.lm(emb)\n",
    "        return self.d(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(name=\"full_lang_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fd.get_full_dataset()\n",
    "ds = tf.data.Dataset.from_tensor_slices(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = ds.batch(32).map(lambda x: (x[:,0], lm.embedding(x[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=\"logs/v1\", update_freq=1000, write_images=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = lm.fit(tds.prefetch(1), epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what the model has learned (eg are there any relationships in vector space that are interesting?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = tf.data.Dataset.from_tensor_slices([x[1] for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things = [x for x in stuff.batch(100).map(lm.embedding)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_emb = tf.concat(things, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = tf.keras.layers.Dot(name=\"sim\", axes=-1, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lm.embedding(tf.constant([\"cat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = dot([p, tf.expand_dims(all_words_emb, 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[data[i] for i in tf.math.top_k(tf.expand_dims(tf.squeeze(sim), 0), 25).indices.numpy()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.math.top_k(sim, 5).indices.numpy()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
